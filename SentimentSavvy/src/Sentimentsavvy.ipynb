import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from joblib import Parallel, delayed

# Load the dataset
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IMDB_Dataset.csv')

# Data exploration and visualization
print("Sentiment Distribution:")
print(data['sentiment'].value_counts())
sns.countplot(x='sentiment', data=data)
plt.show()

# Text preprocessing
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = text.lower()
    text = ' '.join([word for word in text.split() if word not in stop_words])
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])
    return text

data['review'] = Parallel(n_jobs=-1)(delayed(preprocess_text)(text) for text in data['review'])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)

# TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Map string labels to numeric values
y_train_numeric = [1 if label == 'positive' else 0 for label in y_train]
y_test_numeric = [1 if label == 'positive' else 0 for label in y_test]

# Define a pipeline for Logistic Regression
logreg_pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=5000)),
    ('logreg', LogisticRegression(max_iter=1000))
])

# Train and evaluate Logistic Regression model
logreg_pipeline.fit(X_train, y_train_numeric)
y_pred_logreg = logreg_pipeline.predict(X_test)

print("Logistic Regression Performance:")
print("Accuracy:", accuracy_score(y_test_numeric, y_pred_logreg))
print("F1-score:", f1_score(y_test_numeric, y_pred_logreg))
print("Precision:", precision_score(y_test_numeric, y_pred_logreg))
print("Recall:", recall_score(y_test_numeric, y_pred_logreg))
print("ROC-AUC:", roc_auc_score(y_test_numeric, y_pred_logreg))

# Hyperparameter tuning using RandomizedSearchCV
param_grid = {'logreg__C': [0.1, 1, 10], 'logreg__penalty': ['l2'], 'logreg__solver': ['liblinear', 'saga']}
random_search = RandomizedSearchCV(logreg_pipeline, param_grid, cv=5, n_iter=5, n_jobs=-1)
random_search.fit(X_train, y_train_numeric)
print("Best Parameters:", random_search.best_params_)
print("Best Score:", random_search.best_score_)

# Define a pipeline for Random Forest
rf_pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=5000)),
    ('rf', RandomForestClassifier(n_estimators=100))
])

# Train and evaluate Random Forest model
rf_pipeline.fit(X_train, y_train_numeric)
y_pred_rf = rf_pipeline.predict(X_test)

print("Random Forest Performance:")
print("Accuracy:", accuracy_score(y_test_numeric, y_pred_rf))
print("F1-score:", f1_score(y_test_numeric, y_pred_rf))
print("Precision:", precision_score(y_test_numeric, y_pred_rf))
print("Recall:", recall_score(y_test_numeric, y_pred_rf))
print("ROC-AUC:", roc_auc_score(y_test_numeric, y_pred_rf))

# Save the trained model and vectorizer
joblib.dump(random_search.best_estimator_, 'sentiment_analysis_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

def predict_sentiment(text):
    text = preprocess_text(text)  # Preprocess the text
    prediction = random_search.best_estimator_.predict([text])
    return 'Positive' if prediction[0] == 1 else 'Negative'

# Test the function
print(predict_sentiment('A wonderful little production.The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only "has got all the polari" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \'dream\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\'s murals decorating every surface) are terribly well done.'))
print(predict_sentiment('This show was an amazing, fresh & innovative idea in the 70\'s when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it\'s continued its decline further to the complete waste of time it is today.<br /><br />It\'s truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn\'t still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can\'t believe it\'s still on the air.'))

